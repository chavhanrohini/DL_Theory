{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0c93a1",
   "metadata": {},
   "source": [
    "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "   - Stateful RNN:\n",
    "     - Pros:\n",
    "       - Suitable for tasks where the order and continuity of sequences matter, such as time series forecasting.\n",
    "       - Maintains memory across batches, allowing the model to learn long-term dependencies.\n",
    "     - Cons:\n",
    "       - Typically requires manual handling of state resetting between sequences.\n",
    "       - May suffer from vanishing/exploding gradient problems in very deep models.\n",
    "   - Stateless RNN:\n",
    "     - Pros:\n",
    "       - Simpler to implement and train since each batch is independent.\n",
    "       - Works well for tasks where sequence order is not critical, such as text classification.\n",
    "     - Cons:\n",
    "       - May struggle to capture long-term dependencies and continuity in sequences.\n",
    "\n",
    "\n",
    "2. Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "   - Encoder–Decoder RNNs are favored for translation tasks because they are designed to handle variable-length input and output sequences. The encoder processes the input sequence and generates a fixed-length representation (context vector), which the decoder then uses to generate the output sequence. This architecture is well-suited for translation, where input and output sequences can have different lengths.\n",
    "\n",
    "\n",
    "3. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "   - To deal with variable-length input sequences, you can use techniques like padding, which involves adding placeholder values (e.g., zeros) to make all sequences in a batch have the same length. Masking can be applied to ignore padded values during processing.\n",
    "   - For variable-length output sequences, you can use techniques like teacher forcing (providing ground-truth outputs as inputs during training) and dynamic decoding during inference to handle sequences of different lengths.\n",
    "\n",
    "\n",
    "4. What is beam search and why would you use it? What tool can you use to implement it?\n",
    "   - Beam search is a search algorithm used in sequence generation tasks, such as machine translation and text generation. It explores multiple possible sequences in parallel and selects the most likely candidates based on a scoring mechanism. Beam search helps generate more coherent and contextually relevant sequences compared to greedy decoding.\n",
    "   - You can implement beam search in TensorFlow or other deep learning frameworks using custom code. Beam search is often used in combination with models like the Transformer for better sequence generation results.\n",
    "\n",
    "\n",
    "5. What is an attention mechanism? How does it help?\n",
    "   - An attention mechanism is a component of neural network architectures, such as the Transformer, that allows models to focus on different parts of the input sequence when making predictions. It helps capture long-range dependencies and relationships between elements in a sequence. Attention mechanisms enable the model to give varying levels of importance to different parts of the input, improving its ability to generate contextually relevant output.\n",
    "\n",
    "\n",
    "6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
    "   - The most important layer in the Transformer architecture is the \"Multi-Head Self-Attention\" layer. Its purpose is to allow the model to attend to different positions in the input sequence and to different parts of the context when making predictions. This layer captures dependencies and relationships between elements in a sequence, enabling the Transformer to handle sequential data effectively.\n",
    "\n",
    "\n",
    "7. When would you need to use sampled softmax?\n",
    "   - Sampled softmax is used in situations where a traditional softmax computation becomes computationally expensive due to a large number of classes or a high-dimensional output space. It is commonly used in tasks like natural language processing, where the vocabulary size can be very large. Sampled softmax reduces computational complexity by approximating the full softmax with a subset of randomly sampled target classes. This technique is used during training to speed up the training process while maintaining competitive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a6e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
