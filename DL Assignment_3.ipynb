{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960b3473",
   "metadata": {},
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "   No, it is not okay to initialize all the weights to the same value even if that value is selected randomly using He initialization. He initialization, which sets initial weights as random values with a variance of 2 divided by the number of input units, is designed to prevent the vanishing gradient problem and accelerate training in deep neural networks. Initializing all weights to the same value would still result in the same problem of symmetry breaking and hinder the network's ability to learn diverse features.\n",
    "\n",
    "2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "   Yes, it is generally okay to initialize bias terms to 0. Bias terms provide the network with the flexibility to fit the data by shifting the activation function horizontally. Initializing them to 0 is a common practice because it ensures that the neurons start with no initial bias towards any particular direction. However, some variations in bias initialization, such as small random values, may also be used depending on the specific problem and architecture.\n",
    "\n",
    "3. Name three advantages of the SELU activation function over ReLU:\n",
    "   - Self-Normalization: SELU is designed to promote self-normalization in deep networks, which means it helps maintain a consistent mean and variance of activations during training, addressing the vanishing/exploding gradient problem.\n",
    "   - Avoids Dying Neurons: SELU is less prone to \"dying\" neurons (neurons that stop learning) compared to ReLU because it allows for negative values, ensuring that gradients can flow in both directions.\n",
    "   - Improved Gradient Flow: SELU has a smoother derivative than ReLU, leading to improved gradient flow and convergence during training.\n",
    "\n",
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "   - SELU: Use SELU in deep neural networks when you want to achieve self-normalization, avoid vanishing/exploding gradients, and benefit from faster convergence.\n",
    "   - Leaky ReLU (and variants): Leaky ReLU, Parametric ReLU (PReLU), and Exponential Linear Unit (ELU) can be used when you want to mitigate the dying ReLU problem and allow a small gradient to flow for negative inputs. They are generally suitable for most scenarios.\n",
    "   - ReLU: ReLU is a good default choice for activation functions in hidden layers when you want simplicity, efficiency, and faster training.\n",
    "   - tanh: Use tanh when you need outputs in the range (-1, 1), such as in the hidden layers of an MLP. It helps mitigate vanishing gradients, similar to sigmoid.\n",
    "   - Logistic (Sigmoid): Sigmoid is typically used in the output layer for binary classification problems, where you need to produce probability values between 0 and 1.\n",
    "   - Softmax: Softmax is used in the output layer for multi-class classification problems. It converts raw scores into class probabilities.\n",
    "\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "   Setting the momentum hyperparameter very close to 1 in stochastic gradient descent (SGD) can lead to slow convergence and oscillations in the training process. When momentum is too high, the optimizer accumulates a large portion of the previous gradients, which can cause it to overshoot the optimal weights and get stuck in a zigzag pattern near the minimum of the loss function. This can significantly slow down the training process, making it less effective. Properly tuning the momentum value is important to balance exploration and exploitation in gradient descent.\n",
    "\n",
    "6. Name three ways you can produce a sparse model:\n",
    "   - L1 Regularization (Lasso): Apply L1 regularization to the model's loss function. This encourages many of the model's weights to become exactly zero, resulting in a sparse model.\n",
    "   - Dropout: Dropout is a regularization technique that randomly sets a fraction of neurons to zero during training. While not exactly producing a sparse model, it can have a similar effect of reducing the reliance on individual neurons, making the model more robust and potentially less complex.\n",
    "   - Pruning: Pruning involves removing connections (weights) that have low importance based on criteria such as weight magnitude or activation values. This can lead to a sparser neural network architecture.\n",
    "\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "   - Dropout: Dropout can slow down training because it introduces randomness and effectively reduces the capacity of the network during each training iteration. However, it often results in a more robust and regularized model, which can help generalize better to unseen data. Inference (making predictions on new instances) with dropout is typically fast because dropout is turned off during inference, and the entire network is used for prediction.\n",
    "   \n",
    "   - MC Dropout (Monte Carlo Dropout): MC Dropout involves making multiple predictions with dropout turned on and averaging the results. This can slow down inference since you need to run the network multiple times with dropout enabled. However, it can provide better uncertainty estimates for predictions, which can be valuable in certain applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b57555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
