{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaceae4a",
   "metadata": {},
   "source": [
    "1. Explain One-Hot Encoding:\n",
    "   - One-Hot Encoding is a technique used to represent categorical data, such as words or labels, as binary vectors. Each category is assigned a unique binary vector where only one element (corresponding to the category) is '1', and all others are '0'. This representation allows machine learning models to work with categorical data as input. One-Hot Encoding is commonly used in natural language processing (NLP) to encode words or labels for text analysis.\n",
    "\n",
    "2. Explain Bag of Words:\n",
    "   - Bag of Words (BoW) is a simple and fundamental technique for text analysis in NLP. It represents a document as a collection of its constituent words, ignoring word order and structure. BoW constructs a vocabulary of unique words in a corpus and creates a numerical vector for each document, where each dimension corresponds to a word from the vocabulary. The value in each dimension represents the count or presence/absence of the word in the document. BoW is used for tasks like text classification, sentiment analysis, and document retrieval.\n",
    "\n",
    "3. Explain Bag of N-Grams:\n",
    "   - Bag of N-Grams is an extension of the Bag of Words (BoW) model that includes not only single words but also sequences of 'n' consecutive words from a text. N-grams capture some degree of word order and context, which is lost in the traditional BoW model. For example, a 2-gram (bigram) model would consider pairs of adjacent words in the text. Bag of N-Grams is particularly useful when preserving some contextual information is necessary for text analysis.\n",
    "\n",
    "4. Explain TF-IDF:\n",
    "   - TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used to evaluate the importance of a word within a document in a collection (corpus). TF-IDF takes into account both the term frequency (how often a word appears in a document) and the inverse document frequency (how unique the word is across the entire corpus). Words that are frequent in a particular document but rare in the overall corpus are given higher TF-IDF scores, making them more indicative of the document's content. TF-IDF is commonly used in information retrieval and text mining.\n",
    "\n",
    "5. What is OOV problem?\n",
    "   - OOV stands for \"Out of Vocabulary\" problem in natural language processing. It refers to the situation where a word that appears in a text or document is not present in the vocabulary or dictionary used by a machine learning model or natural language processing system. Handling the OOV problem involves strategies like using special tokens for unknown words, adding new words to the vocabulary, or employing subword representations to handle unseen or rare words effectively.\n",
    "\n",
    "6. What are word embeddings?\n",
    "   - Word embeddings are dense vector representations of words in a continuous vector space. These representations capture the semantic meaning and relationships between words. Word embeddings are trained using unsupervised methods on large text corpora and are used to convert words into numerical vectors. They have become a fundamental component in many natural language processing tasks, as they allow models to capture word semantics and context.\n",
    "\n",
    "7. Explain Continuous bag of words (CBOW):\n",
    "   - Continuous Bag of Words (CBOW) is a word embedding model used in natural language processing. It aims to predict a target word based on its surrounding context words. CBOW takes a fixed-size context window of words and learns to predict the center word. This model is particularly useful for creating word embeddings, where each word is represented as a dense vector capturing its semantic meaning based on the words around it.\n",
    "\n",
    "8. Explain SkipGram:\n",
    "   - SkipGram is another word embedding model that operates in the reverse way of CBOW. Instead of predicting a target word from context words, SkipGram predicts context words from a target word. It is a popular choice for training word embeddings when the focus is on the context words and their relationships with the target word. SkipGram is known for its ability to capture semantic meaning and relationships between words.\n",
    "\n",
    "9. Explain Glove Embeddings:\n",
    "   - GloVe, which stands for Global Vectors for Word Representation, is a word embedding model that combines the best of both CBOW and SkipGram approaches. It leverages global word co-occurrence statistics to create word embeddings. GloVe aims to capture the meaning and relationships between words based on their co-occurrence patterns across a large text corpus. It is widely used for a variety of NLP tasks and has gained popularity for its efficiency and accuracy in capturing word semantics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff415f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
