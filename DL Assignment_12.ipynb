{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545b4c34",
   "metadata": {},
   "source": [
    "1. How does `unsqueeze` help us to solve certain broadcasting problems?\n",
    "\n",
    "   - The `unsqueeze` function in PyTorch (or similar operations in other frameworks) is used to increase the dimensionality of a tensor. It helps in solving broadcasting problems by adding new dimensions to a tensor so that its shape aligns with another tensor for element-wise operations. Broadcasting allows operations between tensors of different shapes, and `unsqueeze` is a way to reshape tensors to make them compatible for such operations.\n",
    "\n",
    "2. How can we use indexing to do the same operation as `unsqueeze`?\n",
    "\n",
    "   - You can use indexing to achieve the same result as `unsqueeze` by selecting a specific dimension and creating a new axis with size 1. For example, if you have a 1D tensor `x` and you want to add a new axis to make it a 2D tensor, you can use indexing like this: `x[:, None]` or `x[:, np.newaxis]` in NumPy. This adds a new axis along the second dimension, effectively performing the same operation as `unsqueeze(1)`.\n",
    "\n",
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "   - To display the actual contents of a tensor in Python (e.g., in NumPy or PyTorch), you can simply print the tensor variable. For example, in PyTorch, you can use `print(tensor)` to display the tensor's values.\n",
    "\n",
    "4. When adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix?\n",
    "\n",
    "   - When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each row of the matrix. This is a broadcasting operation, where the vector is extended to match the shape of the matrix by repeating it along the rows, and then element-wise addition is performed.\n",
    "\n",
    "5. Do broadcasting and `expand_as` result in increased memory use? Why or why not?\n",
    "\n",
    "   - Broadcasting and `expand_as` do not result in increased memory use for the tensors involved. These operations are performed without replicating the data in memory. Instead, they change the way the tensors are \"viewed\" or \"interpreted\" during computation. This makes them memory-efficient as they avoid unnecessary data duplication.\n",
    "\n",
    "6. Implement `matmul` using Einstein summation.\n",
    "\n",
    "   - The `matmul` operation can be implemented using Einstein summation notation as follows:\n",
    "   ```python\n",
    "   result = np.einsum(\"ij,jk->ik\", matrix1, matrix2)\n",
    "   ```\n",
    "   This notation specifies that the matrix multiplication is performed between `matrix1` (size NxM) and `matrix2` (size MxK) to produce the result `result` (size NxK).\n",
    "\n",
    "7. What does a repeated index letter represent on the lefthand side of `einsum`?\n",
    "\n",
    "   - In Einstein summation notation, when an index letter is repeated on the left-hand side, it indicates a summation or contraction along that axis. For example, if you have `\"ij,jk->ik\"`, it means that you are summing over the \"j\" axis of the first tensor and the \"j\" axis of the second tensor to produce the \"ik\" result.\n",
    "\n",
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "   - The three rules of Einstein summation notation are:\n",
    "     1. Repeated indices are summed over.\n",
    "     2. Unmatched indices in the output represent new dimensions.\n",
    "     3. Matched indices in the input and output indicate element-wise multiplication.\n",
    "   These rules simplify the notation of complex tensor operations and provide a concise and expressive way to describe operations, making it easier to understand and compute complex tensor operations.\n",
    "\n",
    "9. What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "   - The forward pass of a neural network is the process of feeding input data through the network to obtain predictions or output. It involves applying a series of mathematical operations, such as matrix multiplications, activations, and loss computations, layer by layer, until the final output is generated.\n",
    "   - The backward pass, often referred to as backpropagation, is the process of computing gradients with respect to the network's parameters. It's used for training the neural network through gradient descent optimization. Backpropagation calculates how the loss changes concerning each parameter and updates the parameters accordingly to minimize the loss during training.\n",
    "\n",
    "10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "\n",
    "    - Storing intermediate activations during the forward pass is necessary for backpropagation and training the neural network. During the backward pass, these stored activations are used to compute gradients with respect to the network's parameters. Storing activations helps in calculating the gradients efficiently and accurately, allowing the network to learn from the training data.\n",
    "\n",
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "    - The downside of having activations with a standard deviation too far away from 1 is that it can lead to issues during training, especially when using activation functions like sigmoid or tanh. If the standard deviation is too small, it may result in vanishing gradients, making training slow and convergence difficult. On the other hand, if the standard deviation is too large, it may lead to exploding gradients, causing numerical instability during training. Proper weight initialization techniques are used to mitigate these problems and keep the standard deviations of activations within reasonable bounds.\n",
    "\n",
    "12. How can weight initialization help avoid this problem?\n",
    "\n",
    "    - Weight initialization is a crucial technique in deep learning that can help avoid issues related to activations with standard deviations that are too far from 1. Proper weight initialization methods set initial values for network weights in a way that prevents vanishing or exploding gradients during training. Methods like He initialization, Xavier initialization, and others are designed to maintain activations with standard deviations that are close to 1, promoting more stable and efficient training of deep neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27be430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
