{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c8f829",
   "metadata": {},
   "source": [
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "   - No, it is not okay to initialize all the weights to the same value, even if that value is randomly selected using He initialization. Weight symmetry can still be a problem even with good initialization methods. Randomly initializing the weights is crucial to break the symmetry, as it ensures that different neurons learn different features.\n",
    "\n",
    "2. Is it okay to initialize the bias terms to 0?\n",
    "   - Initializing bias terms to 0 is a common practice and is generally acceptable. However, it's important to note that the effect of bias terms is to shift the activation function and introduce an offset. In some cases, depending on the network architecture and the specific problem, initializing bias terms to non-zero values might be beneficial. Nonetheless, initializing bias terms to 0 is a reasonable default choice.\n",
    "\n",
    "3. Name three advantages of the ELU activation function over ReLU.\n",
    "   - Three advantages of the Exponential Linear Unit (ELU) activation function over the Rectified Linear Unit (ReLU) are:\n",
    "     1. Smoothness: ELU is a smooth activation function, which mitigates the vanishing gradient problem and allows for gradient-based optimization to proceed more smoothly during training.\n",
    "     2. Robust to Dead Neurons: ELU is less prone to the \"dying ReLU\" problem, where some neurons never activate during training, as it can take both positive and negative values.\n",
    "     3. Captures Negative Values: ELU can model negative values in the data, making it suitable for a broader range of input distributions compared to ReLU.\n",
    "\n",
    "4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "   - ELU: ELU is a good choice for hidden layers in deep neural networks, especially when you want to mitigate the vanishing gradient problem and handle a wide range of input distributions.\n",
    "   - Leaky ReLU (and its variants): Leaky ReLU and variants like Parametric ReLU (PReLU) can be useful when you want to address the \"dying ReLU\" problem and need a simple and computationally efficient activation function.\n",
    "   - ReLU: ReLU is a popular choice for most hidden layers in deep neural networks due to its simplicity and computational efficiency. However, it can be sensitive to weight initialization and might not be suitable for all types of data distributions.\n",
    "   - tanh: The hyperbolic tangent (tanh) activation function is often used for hidden layers in recurrent neural networks (RNNs) and convolutional neural networks (CNNs) due to its zero-centered nature.\n",
    "   - logistic (Sigmoid): The logistic (Sigmoid) activation function is commonly used in binary classification tasks for the output layer to model class probabilities.\n",
    "   - softmax: The softmax activation function is suitable for the output layer in multi-class classification tasks, as it normalizes the class scores into probabilities.\n",
    "\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "   - Setting the momentum hyperparameter too close to 1, such as 0.99999, can lead to issues in training. Momentum is used to accelerate convergence and escape local minima during optimization. If the momentum is set too close to 1, it can cause the optimization process to oscillate, overshoot, or even diverge, making training unstable. Proper values for momentum typically fall in the range of 0.9 to 0.99, where it strikes a balance between convergence speed and stability.\n",
    "\n",
    "6. Name three ways you can produce a sparse model.\n",
    "   - Three ways to produce a sparse model (a model with a substantial number of weights set to zero) include:\n",
    "     1. Pruning: Pruning involves iteratively removing or setting to zero the least important weights in a trained model, based on various criteria like weight magnitude, sensitivity analysis, or using techniques like the L1 regularization (Lasso).\n",
    "     2. Sparsity Regularization: Regularization techniques like L1 regularization (Lasso) encourage sparsity by adding a penalty term to the loss function that discourages large weight values. This encourages the model to have many weights close to zero.\n",
    "     3. Sparse Activations: Techniques like dropout and drop connect can lead to sparse activations by randomly deactivating or setting some neurons to zero during training, forcing the network to rely on a subset of its neurons.\n",
    "\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "   - Dropout can slightly slow down training because it introduces randomness and effectively trains an ensemble of models during each forward and backward pass. However, it is a negligible slowdown and is usually outweighed by the regularization benefits and the improved generalization.\n",
    "   - During inference (making predictions on new instances), dropout is typically turned off (i.e., dropout rate is set to 0), so it does not slow down inference. The trained model effectively behaves as a single deterministic model during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e878b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
